# 1. INSTALL DEPENDENCIES
!pip install -q ultralytics roboflow pyyaml

import torch, os, yaml, shutil, random
from roboflow import Roboflow
from ultralytics import YOLO
from IPython.display import Image, display
from google.colab import files

# Check for GPU
if not torch.cuda.is_available():
    print("âš ï¸ GPU not found. Go to Edit -> Notebook Settings -> Select T4 GPU.")
else:
    print(f"âœ… GPU: {torch.cuda.get_device_name(0)}")

# 2. CONFIGURATION & DOWNLOADS
ROBOFLOW_API_KEY = "YOUR_API_KEY_HERE"
rf = Roboflow(api_key=ROBOFLOW_API_KEY)
datasets = []

print("ðŸ“¥ Downloading datasets...")
datasets.append(("cones", rf.workspace("aimllab-tqjt3").project("debris-6onrk").version(3).download("yolov11")))
datasets.append(("debris", rf.workspace("old-dataset").project("litter-street-images---copy").version(1).download("yolov11")))
datasets.append(("trashcans", rf.workspace("volobuevs-workspace").project("trash-cans-segmentation").version(1).download("yolov11")))
# ADDED: Road Damage Dataset
datasets.append(("road_damage", rf.workspace("baka-1ravj").project("road-damage-det").version(1).download("yolov11")))

# Setup directory structure
for split in ["train", "valid", "test"]:
    os.makedirs(f"/content/combined_dataset/{split}/images", exist_ok=True)
    os.makedirs(f"/content/combined_dataset/{split}/labels", exist_ok=True)

all_classes = {}
class_counter = 0

# 3. MERGING LOGIC
def merge_dataset(dataset_name, dataset, split, limit=1000):
    global class_counter, all_classes
    
    yaml_path = os.path.join(dataset.location, "data.yaml")
    with open(yaml_path, "r") as f:
        data = yaml.safe_load(f)

    img_dir = os.path.join(dataset.location, split, "images")
    lbl_dir = os.path.join(dataset.location, split, "labels")
    if not os.path.exists(img_dir): return 0

    all_images = os.listdir(img_dir)
    random.shuffle(all_images) # Randomize dataset selection
    selected_images = all_images[:limit] # Limit to 1000 images

    count = 0
    for img_name in selected_images:
        lbl_name = img_name.rsplit(".", 1)[0] + ".txt"
        lbl_path = os.path.join(lbl_dir, lbl_name)
        if not os.path.exists(lbl_path): continue

        with open(lbl_path, "r") as f:
            lines = f.readlines()

        new_lines = []
        for line in lines:
            parts = line.strip().split()
            if len(parts) < 5: continue
            
            original_class = data["names"][int(parts[0])]

            # --- CUSTOM RULES ---
            if dataset_name == "cones":
                if original_class.lower() in ["cone", "tube"]: final_name = "cone"
                else: continue
            elif dataset_name == "debris":
                final_name = f"debris_{original_class}" # Keeps original classes with a prefix
            elif dataset_name == "road_damage":
                final_name = original_class # Keeps all road damage classes (pothole, cracks, etc)
            elif dataset_name == "trashcans":
                if original_class.lower() == "empty": final_name = "open_trash_can"
                else: continue
            else: continue

            if final_name not in all_classes:
                all_classes[final_name] = class_counter
                class_counter += 1

            new_lines.append(f"{all_classes[final_name]} {' '.join(parts[1:])}\n")

        if new_lines:
            shutil.copy(os.path.join(img_dir, img_name), f"/content/combined_dataset/{split}/images/{dataset_name}_{img_name}")
            with open(f"/content/combined_dataset/{split}/labels/{dataset_name}_{lbl_name}", "w") as f:
                f.writelines(new_lines)
            count += 1
    return count

print("ðŸ› ï¸ Merging and Randomizing Data...")
for name, ds in datasets:
    for s in ["train", "valid", "test"]:
        num = merge_dataset(name, ds, s)
    print(f"   -> {name} merged (capped at 1000 per split)")

# Create unified data.yaml
data_config = {
    "path": "/content/combined_dataset",
    "train": "train/images", "val": "valid/images", "test": "test/images",
    "nc": len(all_classes),
    "names": list(all_classes.keys())
}
with open("/content/combined_dataset/data.yaml", "w") as f:
    yaml.dump(data_config, f)

# 4. TRAINING
# Loading yolo11n.pt starts from scratch; change to your "best.pt" to continue training
model = YOLO("yolo11n.pt") 
model.train(
    data="/content/combined_dataset/data.yaml",
    epochs=30,
    imgsz=640,
    batch=16,
    device=0,
    name="vancouver_hazards_v2"
)

# 5. TEST & DOWNLOAD
print("ðŸ“¦ Zipping weights for download...")
!zip -j vancouver_hazards.zip /content/runs/detect/vancouver_hazards_v2/weights/best.pt
files.download("vancouver_hazards.zip")

print(f"ðŸ”¥ DONE. Classes: {list(all_classes.keys())}")
