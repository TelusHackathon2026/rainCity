# ============================================================
# Vancouver Storm Drain Hazard Detector - YOLOv11
# Using YOLO version 11 (not v8)
# ============================================================

print("Starting YOLOv11 Training Pipeline\n")

# Step 1: Install required packages
print("Installing latest Ultralytics library for YOLOv11 support...")
!pip install -q -U ultralytics roboflow pyyaml
print("Installation complete.\n")

import os, yaml, shutil, random
from roboflow import Roboflow
from ultralytics import YOLO
from google.colab import files
import torch
from collections import Counter

# Verify version (must be 8.3.0+ for YOLOv11)
from ultralytics import __version__
print(f"Ultralytics version: {__version__}")
print("Note: YOLOv11 requires version 8.3.0 or higher\n")

# Step 2: Check GPU availability
print("Checking GPU availability...")
if not torch.cuda.is_available():
    print("ERROR: GPU not found! Please enable GPU in Runtime -> Change runtime type -> T4 GPU")
    raise Exception("GPU required")
else:
    print(f"GPU detected: {torch.cuda.get_device_name(0)}")
    print(f"VRAM available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\n")

# Step 3: Download datasets from Roboflow
print("=" * 60)
print("Downloading datasets from Roboflow")
print("=" * 60 + "\n")

rf = Roboflow(api_key="7OEDxnVQ6Ycy7pwHBxLX")

datasets = []

# Dataset 1: Traffic Cones
print("Dataset 1: Downloading Traffic Cones (debris-6onrk)...")
try:
    project1 = rf.workspace("aimllab-tqjt3").project("debris-6onrk")
    version1 = project1.version(1)
    dataset1 = version1.download("yolov11")
    datasets.append(("cones", dataset1))
    train_count = len(os.listdir(f"{dataset1.location}/train/images"))
    print(f"   Downloaded: {train_count} images\n")
except Exception as e:
    print(f"   Error: {e}\n")

# Dataset 2: Street Debris
print("Dataset 2: Downloading Street Debris (litter-street-images)...")
try:
    project2 = rf.workspace("kabml-images").project("litter-street-images")
    version2 = project2.version(10)
    dataset2 = version2.download("yolov11")
    datasets.append(("debris", dataset2))
    train_count = len(os.listdir(f"{dataset2.location}/train/images"))
    print(f"   Downloaded: {train_count} images\n")
except Exception as e:
    print(f"   Error: {e}\n")

# Dataset 3: Fallen Trees
print("Dataset 3: Downloading Fallen Trees (fallen-trees-detection-v1_new_val)...")
try:
    project3 = rf.workspace("erykvtymons").project("fallen-trees-detection-v1_new_val")
    version3 = project3.version(3)
    dataset3 = version3.download("yolov11")
    datasets.append(("fallen_trees", dataset3))
    train_count = len(os.listdir(f"{dataset3.location}/train/images"))
    print(f"   Downloaded: {train_count} images\n")
except Exception as e:
    print(f"   Error: {e}\n")

# Dataset 4: Crashed Cars
print("Dataset 4: Downloading Crashed Cars (crash-car-7ld4n)...")
try:
    project4 = rf.workspace("f-rid-nagiyev").project("crash-car-7ld4n")
    version4 = project4.version(1)
    dataset4 = version4.download("yolov11")
    datasets.append(("crashed_cars", dataset4))
    train_count = len(os.listdir(f"{dataset4.location}/train/images"))
    print(f"   Downloaded: {train_count} images\n")
except Exception as e:
    print(f"   Error: {e}\n")

# Dataset 5: Fallen People
print("Dataset 5: Downloading Fallen People (fallen-people-9wcfl)...")
try:
    project5 = rf.workspace("fallenpeople-training-dataset-complete-computer-vision-project").project("fallen-people-9wcfl")
    version5 = project5.version(18)
    dataset5 = version5.download("yolov11")
    datasets.append(("fallen_people", dataset5))
    train_count = len(os.listdir(f"{dataset5.location}/train/images"))
    print(f"   Downloaded: {train_count} images\n")
except Exception as e:
    print(f"   Error: {e}\n")

# Dataset 6: Pedestrians
print("Dataset 6: Downloading Pedestrians (pedestrians-ydiae)...")
try:
    project6 = rf.workspace("ece479").project("pedestrians-ydiae")
    version6 = project6.version(2)
    dataset6 = version6.download("yolov11")
    datasets.append(("pedestrians", dataset6))
    train_count = len(os.listdir(f"{dataset6.location}/train/images"))
    print(f"   Downloaded: {train_count} images\n")
except Exception as e:
    print(f"   Error: {e}\n")

# Dataset 7: Vehicles
print("Dataset 7: Downloading Vehicles (vehicle-counting-ev2em)...")
try:
    project7 = rf.workspace("ml-mvw1s").project("vehicle-counting-ev2em")
    version7 = project7.version(1)
    dataset7 = version7.download("yolov11")
    datasets.append(("vehicles", dataset7))
    train_count = len(os.listdir(f"{dataset7.location}/train/images"))
    print(f"   Downloaded: {train_count} images\n")
except Exception as e:
    print(f"   Error: {e}\n")

if not datasets:
    raise Exception("ERROR: No datasets downloaded! Check API key and internet connection.")

print(f"Successfully downloaded {len(datasets)}/7 datasets\n")

# Step 4: Create combined dataset folder structure
print("Creating combined dataset structure...")
for split in ["train", "valid", "test"]:
    os.makedirs(f"/content/combined/{split}/images", exist_ok=True)
    os.makedirs(f"/content/combined/{split}/labels", exist_ok=True)
print("Folder structure created.\n")

# Step 5: Merge datasets with class remapping
print("=" * 60)
print("Merging datasets into unified multi-class dataset")
print("=" * 60 + "\n")

all_classes = {}
class_counter = 0

def merge_dataset(dataset_name, dataset, split, limit=1000):
    """Merge Roboflow dataset into combined folder with class remapping."""
    global class_counter, all_classes
    
    yaml_path = os.path.join(dataset.location, "data.yaml")
    if not os.path.exists(yaml_path):
        print(f"   Warning: {dataset_name} - No data.yaml found")
        return 0

    with open(yaml_path, "r") as f:
        data = yaml.safe_load(f)

    img_dir = os.path.join(dataset.location, split, "images")
    lbl_dir = os.path.join(dataset.location, split, "labels")
    
    if not os.path.exists(img_dir):
        return 0

    all_images = os.listdir(img_dir)
    random.shuffle(all_images)
    selected_images = all_images[:limit]

    count = 0
    
    for img_name in selected_images:
        lbl_name = os.path.splitext(img_name)[0] + ".txt"
        lbl_path = os.path.join(lbl_dir, lbl_name)
        
        if not os.path.exists(lbl_path):
            continue

        with open(lbl_path, "r") as f:
            lines = f.readlines()

        new_lines = []
        for line in lines:
            parts = line.strip().split()
            if len(parts) < 5:
                continue
            
            original_class_id = int(parts[0])
            if original_class_id >= len(data["names"]):
                continue
                
            original_class = data["names"][original_class_id]

            # Class mapping rules
            final_name = None
            
            if dataset_name == "cones":
                # debris-6onrk: extract cones and other debris
                if any(word in original_class.lower() for word in ["cone", "tube", "traffic"]):
                    final_name = "traffic_cone"
                else:
                    clean = original_class.lower().replace(' ', '_').replace('-', '_')
                    final_name = f"debris_{clean}"
                    
            elif dataset_name == "debris":
                # litter-street-images: all street debris
                clean = original_class.lower().replace(' ', '_').replace('-', '_')
                final_name = f"debris_{clean}"
                
            elif dataset_name == "fallen_trees":
                # Map to fallen_tree
                final_name = "fallen_tree"
                
            elif dataset_name == "crashed_cars":
                # Map to crashed_car
                final_name = "crashed_car"
                
            elif dataset_name == "fallen_people":
                # Map to fallen_person
                final_name = "fallen_person"
                
            elif dataset_name == "pedestrians":
                # Map to pedestrian
                final_name = "pedestrian"
                
            elif dataset_name == "vehicles":
                # Map vehicle types
                lower_class = original_class.lower()
                if "car" in lower_class:
                    final_name = "car"
                elif "truck" in lower_class:
                    final_name = "truck"
                elif "bus" in lower_class:
                    final_name = "bus"
                elif "motorcycle" in lower_class or "bike" in lower_class:
                    final_name = "motorcycle"
                else:
                    final_name = "vehicle"

            if final_name is None:
                continue

            # Add to class dictionary
            if final_name not in all_classes:
                all_classes[final_name] = class_counter
                class_counter += 1

            # Write remapped label
            new_lines.append(f"{all_classes[final_name]} {' '.join(parts[1:])}\n")

        # Copy image and label if valid
        if new_lines:
            new_img_name = f"{dataset_name}_{img_name}"
            shutil.copy(
                os.path.join(img_dir, img_name),
                f"/content/combined/{split}/images/{new_img_name}"
            )
            
            new_lbl_name = f"{dataset_name}_{lbl_name}"
            with open(f"/content/combined/{split}/labels/{new_lbl_name}", "w") as f:
                f.writelines(new_lines)
            
            count += 1
            
    return count

# Merge all datasets
total_merged = 0
for name, ds in datasets:
    print(f"Processing {name}...")
    dataset_total = 0
    for split in ["train", "valid", "test"]:
        count = merge_dataset(name, ds, split, limit=1000)
        dataset_total += count
        if count > 0:
            print(f"   {split}: {count} images")
    
    total_merged += dataset_total
    print(f"   Total: {dataset_total} images\n")

print("=" * 60)
print(f"Merge complete")
print(f"   Total images: {total_merged}")
print(f"   Total classes: {len(all_classes)}")
print("=" * 60 + "\n")

print("Final class list:")
for cls_name in sorted(all_classes.keys()):
    print(f"   - {cls_name}")
print()

# Step 6: Create data.yaml configuration file
print("Creating data.yaml for YOLOv11...")

data_config = {
    "path": "/content/combined",
    "train": "train/images",
    "val": "valid/images",
    "test": "test/images",
    "nc": len(all_classes),
    "names": list(all_classes.keys())
}

with open("/content/combined/data.yaml", "w") as f:
    yaml.dump(data_config, f, default_flow_style=False)

print("data.yaml created successfully.\n")

print("Configuration:")
print("-" * 40)
with open("/content/combined/data.yaml", "r") as f:
    print(f.read())
print("-" * 40 + "\n")

# Step 7: Train YOLOv11 model
print("=" * 60)
print("Training YOLOv11 Model")
print("=" * 60)
print("Model: YOLO11n (YOLOv11 Nano)")
print("Epochs: 30")
print("Image size: 640")
print("Batch size: 16")
print("Estimated time: 20-35 minutes\n")

# Load YOLOv11n model
model = YOLO("yolo11n.pt")

try:
    results = model.train(
        data="/content/combined/data.yaml",
        epochs=30,
        imgsz=640,
        batch=16,
        device=0,
        workers=8,
        cache=True,
        patience=10,
        name="vancouver_hazards_v11",
        verbose=True,
        plots=True
    )
    
    print("\n" + "=" * 60)
    print("YOLOv11 Training Complete")
    print("=" * 60 + "\n")
    
except Exception as e:
    print(f"\nTraining error: {e}")
    import traceback
    traceback.print_exc()
    raise

# Step 8: Validate model performance
print("Validating YOLOv11 model...\n")

try:
    metrics = model.val()
    
    print("=" * 60)
    print("YOLOv11 Validation Metrics")
    print("=" * 60)
    print(f"mAP50:     {metrics.box.map50:.3f}")
    print(f"mAP50-95:  {metrics.box.map:.3f}")
    print(f"Precision: {metrics.box.mp:.3f}")
    print(f"Recall:    {metrics.box.mr:.3f}")
    print("=" * 60 + "\n")
    
except Exception as e:
    print(f"Validation error: {e}\n")

# Step 9: Test inference with object counting
print("Testing YOLOv11 inference with object counting...\n")

def count_objects(results):
    """Count detected objects by class and return detailed statistics."""
    counts = Counter()
    total_objects = 0
    
    for result in results:
        for box in result.boxes:
            cls_name = result.names[int(box.cls)]
            counts[cls_name] += 1
            total_objects += 1
    
    return counts, total_objects

try:
    valid_imgs = os.listdir("/content/combined/valid/images")
    if valid_imgs:
        # Test on 5 random images
        num_test_images = min(5, len(valid_imgs))
        test_images = random.sample(valid_imgs, num_test_images)
        
        print(f"Testing on {num_test_images} random validation images:\n")
        
        all_results = []
        
        for idx, img_name in enumerate(test_images, 1):
            test_img = f"/content/combined/valid/images/{img_name}"
            print(f"Image {idx}/{num_test_images}: {img_name}")
            
            results = model(test_img, conf=0.25)
            results[0].save(f"yolov11_test_result_{idx}.jpg")
            all_results.append(results)
            
            # Count objects for this image
            object_counts, total = count_objects(results)
            
            print(f"  Total objects: {total}")
            if total > 0:
                print(f"  Breakdown: {dict(object_counts)}")
            print()
        
        # Aggregate statistics across all test images
        print("=" * 60)
        print("Aggregate Test Statistics")
        print("=" * 60)
        
        total_detections = 0
        combined_counts = Counter()
        
        for results in all_results:
            counts, total = count_objects(results)
            total_detections += total
            combined_counts.update(counts)
        
        print(f"Total objects detected across all images: {total_detections}")
        print(f"Average objects per image: {total_detections / num_test_images:.1f}\n")
        
        if combined_counts:
            print("Combined class counts:")
            for cls_name, count in sorted(combined_counts.items(), key=lambda x: x[1], reverse=True):
                print(f"  - {cls_name}: {count}")
            
            car_count = combined_counts.get('car', 0)
            if car_count > 0:
                print(f"\nTotal cars detected: {car_count}")
        
        print("=" * 60 + "\n")
        
        # Display all test results
        from IPython.display import Image, display
        print("YOLOv11 Detection Results:\n")
        for idx in range(1, num_test_images + 1):
            print(f"Result {idx}:")
            display(Image(f"yolov11_test_result_{idx}.jpg", width=800))
            print()
            
except Exception as e:
    print(f"Test error: {e}")

# Step 10: View training plots
print("\n" + "=" * 60)
print("YOLOv11 Training Plots")
print("=" * 60 + "\n")

from IPython.display import Image, display

results_dir = "/content/runs/detect/vancouver_hazards_v11"

plots = [
    ("results.png", "Training Metrics"),
    ("confusion_matrix.png", "Confusion Matrix"),
    ("F1_curve.png", "F1 Score"),
    ("PR_curve.png", "Precision-Recall")
]

for plot_file, title in plots:
    path = f"{results_dir}/{plot_file}"
    if os.path.exists(path):
        print(f"{title}:")
        display(Image(path, width=800))
        print()

# Step 11: Download trained model
print("=" * 60)
print("Downloading YOLOv11 Trained Model")
print("=" * 60 + "\n")

best_model = f"{results_dir}/weights/best.pt"
last_model = f"{results_dir}/weights/last.pt"

if os.path.exists(best_model):
    model_size = os.path.getsize(best_model) / 1e6
    print(f"Downloading YOLOv11 best.pt ({model_size:.1f} MB)...")
    files.download(best_model)
    print("YOLOv11 best.pt downloaded successfully.\n")
    
    if os.path.exists(last_model):
        print(f"Downloading last.pt (backup)...")
        files.download(last_model)
        print("last.pt downloaded successfully.\n")
else:
    print("ERROR: Model not found!")
    print(f"   Expected: {best_model}")

# Step 12: Create object counting utility function
print("=" * 60)
print("Creating Object Counting Utility")
print("=" * 60 + "\n")

counting_code = '''
from collections import Counter

def analyze_image(model, image_path, conf_threshold=0.25):
    """
    Analyze an image and return object counts and detections.
    
    Args:
        model: Trained YOLO model
        image_path: Path to image file
        conf_threshold: Confidence threshold for detections
        
    Returns:
        dict: {
            'total_objects': int,
            'counts': dict of {class_name: count},
            'car_count': int,
            'detections': list of {class, confidence, bbox}
        }
    """
    results = model(image_path, conf=conf_threshold)
    
    counts = Counter()
    detections = []
    
    for result in results:
        for box in result.boxes:
            cls_name = result.names[int(box.cls)]
            conf = float(box.conf)
            bbox = box.xyxy[0].tolist()  # [x1, y1, x2, y2]
            
            counts[cls_name] += 1
            detections.append({
                'class': cls_name,
                'confidence': conf,
                'bbox': bbox
            })
    
    return {
        'total_objects': sum(counts.values()),
        'counts': dict(counts),
        'car_count': counts.get('car', 0),
        'detections': detections
    }

# Example usage:
# analysis = analyze_image(model, "test_image.jpg")
# print(f"Total objects: {analysis['total_objects']}")
# print(f"Cars: {analysis['car_count']}")
# print(f"All counts: {analysis['counts']}")
'''

with open("/content/object_counting_utils.py", "w") as f:
    f.write(counting_code)

print("Created object_counting_utils.py")
print("This utility can be used for batch processing or integration.\n")

# Step 13: Final summary
print("=" * 60)
print("YOLOv11 Training Complete")
print("=" * 60)

print(f"\nStatistics:")
print(f"   - Model: YOLOv11n (Version 11)")
print(f"   - Total images: {total_merged}")
print(f"   - Total classes: {len(all_classes)}")
print(f"   - Epochs trained: 30")

print(f"\nClasses Detected ({len(all_classes)}):")
for cls_name in sorted(all_classes.keys()):
    print(f"   - {cls_name}")

print(f"\nOutputs:")
print(f"   - Trained model: best.pt (YOLOv11)")
print(f"   - Counting utility: object_counting_utils.py")
print(f"   - Results: {results_dir}")

print("\nNext Steps:")
print("   1. Upload best.pt to Hugging Face")
print("   2. Use object_counting_utils.py for batch analysis")
print("   3. Build frontend with YOLOv11 inference + counts")
print("   4. Track car counts and hazards over time")

print("\nObject Counting Features:")
print("   - Total object count per image")
print("   - Count by class (cars, pedestrians, etc.)")
print("   - Confidence scores for each detection")
print("   - Bounding box coordinates")

print("\n" + "=" * 60)
print("YOLOv11 Model Ready for Deployment")
print("=" * 60)
